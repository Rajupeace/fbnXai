// Enhanced LLM and LangChain Launch Script
const { spawn } = require('child_process');
const path = require('path');

console.log('ðŸ§  Enhanced LLM and LangChain Integration');
console.log('=====================================\n');

// Function to start enhanced server
const startEnhancedServer = () => {
    console.log('ðŸš€ Starting Enhanced VUAI Agent with LLM Integration...');
    
    const serverProcess = spawn('node', ['serverEnhancedLLM.js'], {
        cwd: path.join(__dirname, 'backend'),
        stdio: 'inherit',
        shell: true,
        env: {
            ...process.env,
            NODE_ENV: 'enhanced',
            PORT: 5000
        }
    });
    
    serverProcess.on('error', (error) => {
        console.error('âŒ Failed to start enhanced server:', error.message);
    });
    
    serverProcess.on('close', (code) => {
        console.log(`ðŸ“Š Enhanced server process exited with code ${code}`);
        if (code !== 0) {
            console.log('ðŸ”„ Restarting enhanced server...');
            setTimeout(() => startEnhancedServer(), 2000);
        }
    });
    
    return serverProcess;
};

// Function to test enhanced LLM capabilities
const testEnhancedLLM = async () => {
    console.log('ðŸ§ª Testing Enhanced LLM Capabilities...\n');
    
    const testCases = [
        { message: 'hello', expected: 'greeting' },
        { message: 'help', expected: 'help menu' },
        { message: 'urgent', expected: 'urgent assistance' },
        { message: 'what is ohms law', expected: 'knowledge base' },
        { message: 'calculate 5+3', expected: 'math calculation' },
        { message: 'explain circuits', expected: 'technical explanation' },
        { message: 'python programming', expected: 'programming help' },
        { message: 'database design', expected: 'database knowledge' }
    ];
    
    for (const testCase of testCases) {
        try {
            console.log(`ðŸ” Testing: "${testCase.message}"`);
            
            const response = await fetch('http://localhost:5000/api/llm', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    message: testCase.message,
                    context: { test: true }
                })
            });
            
            if (response.ok) {
                const data = await response.json();
                console.log(`âœ… Success: ${data.response.substring(0, 80)}...`);
                console.log(`âš¡ Response Time: ${data.responseTime}ms`);
                console.log(`ðŸ”„ Source: ${data.source}`);
                console.log(`ðŸ§  Enhanced LLM: ${data.llm ? 'Active' : 'Inactive'}`);
                console.log(`ðŸ’¾ Cached: ${data.cached ? 'Yes' : 'No'}\n`);
            } else {
                console.log(`âŒ Failed: ${response.status}\n`);
            }
            
        } catch (error) {
            console.log(`âŒ Error: ${error.message}\n`);
        }
    }
};

// Function to test emergency fallback
const testEmergencyFallback = async () => {
    console.log('ðŸš¨ Testing Emergency Fallback System...\n');
    
    const emergencyTests = [
        'random message that should trigger emergency',
        '12345',
        '!!!@@@###',
        '',
        'very long message to test emergency system capabilities'
    ];
    
    for (const testMessage of emergencyTests) {
        try {
            console.log(`ðŸ” Testing emergency fallback: "${testMessage.substring(0, 30)}${testMessage.length > 30 ? '...' : ''}"`);
            
            const response = await fetch('http://localhost:5000/api/emergency', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    message: testMessage,
                    context: { emergency: true }
                })
            });
            
            if (response.ok) {
                const data = await response.json();
                console.log(`âœ… Emergency Response: ${data.response.substring(0, 60)}...`);
                console.log(`âš¡ Response Time: ${data.responseTime}ms`);
                console.log(`ðŸ”„ Source: ${data.source}`);
                console.log(`ðŸš¨ Emergency: ${data.emergency ? 'Active' : 'Inactive'}`);
                console.log(`âœ… Guaranteed: ${data.guaranteed ? 'Yes' : 'No'}\n`);
            } else {
                console.log(`âŒ Emergency Failed: ${response.status}\n`);
            }
            
        } catch (error) {
            console.log(`âŒ Emergency Error: ${error.message}\n`);
        }
    }
};

// Function to test system health
const testSystemHealth = async () => {
    console.log('ðŸ¥ Testing Enhanced System Health...\n');
    
    try {
        const response = await fetch('http://localhost:5000/health');
        const health = await response.json();
        
        console.log('âœ… Enhanced Health Check Results:');
        console.log(`ðŸ”Œ Database: ${health.database.status}`);
        console.log(`ðŸ§  Enhanced LLM: ${health.llm.active ? 'Active' : 'Inactive'}`);
        console.log(`ðŸ Python LLM: ${health.llm.pythonReady ? 'Ready' : 'Not Ready'}`);
        console.log(`ðŸ“š Knowledge Bases: ${health.llm.knowledgeBases}`);
        console.log(`âš¡ Fast Responses: ${health.llm.fastResponses}`);
        console.log(`ðŸ’¾ LLM Cache: ${health.llm.cacheSize} items`);
        console.log(`ðŸš¨ Emergency System: ${health.emergency.active ? 'Active' : 'Inactive'}`);
        console.log(`âœ… Guaranteed Responses: ${health.emergency.guaranteed ? 'Yes' : 'No'}`);
        console.log(`ðŸ’¾ Emergency Cache: ${health.emergency.cacheSize} items`);
        console.log(`â±ï¸ Server Uptime: ${Math.floor(health.server.uptime / 60)} minutes\n`);
        
        return health;
        
    } catch (error) {
        console.log(`âŒ Health check failed: ${error.message}\n`);
        return null;
    }
};

// Function to demonstrate LangChain integration
const demonstrateLangChain = async () => {
    console.log('ðŸ”— Demonstrating LangChain Integration...\n');
    
    try {
        const response = await fetch('http://localhost:5000/api/llm/test', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            }
        });
        
        if (response.ok) {
            const testResults = await response.json();
            
            console.log('ðŸ”— LangChain Integration Test Results:');
            console.log(`ðŸ“Š Total Tests: ${testResults.totalTests}`);
            console.log(`âœ… Successful Tests: ${testResults.successfulTests}`);
            console.log(`âš¡ Average Response Time: ${testResults.averageResponseTime}ms`);
            console.log('');
            
            testResults.testResults.forEach((result, index) => {
                console.log(`${index + 1}. "${result.message}"`);
                console.log(`   Response: ${result.response.substring(0, 50)}...`);
                console.log(`   Source: ${result.source}`);
                console.log(`   Time: ${result.responseTime}ms`);
                console.log(`   Cached: ${result.cached ? 'Yes' : 'No'}`);
                console.log('');
            });
            
        } else {
            console.log('âŒ LangChain test failed');
        }
        
    } catch (error) {
        console.log(`âŒ LangChain demonstration error: ${error.message}`);
    }
};

// Main launch function
const launchEnhancedLLM = async () => {
    console.log('ðŸŽ¯ Enhanced LLM and LangChain Features:');
    console.log('=====================================');
    console.log('âœ… Enhanced LLM Integration');
    console.log('âœ… LangChain Integration');
    console.log('âœ… Python Backend Service');
    console.log('âœ… Knowledge Base Integration');
    console.log('âœ… Fast Response System');
    console.log('âœ… Emergency Fallback');
    console.log('âœ… Math Calculations');
    console.log('âœ… Caching System');
    console.log('âœ… Multiple Fallbacks');
    console.log('âœ… Guaranteed Responses');
    console.log('');
    
    // Start server
    console.log('1. Starting enhanced server...');
    const server = startEnhancedServer();
    
    // Wait for server to start
    await new Promise(resolve => setTimeout(resolve, 5000));
    
    // Test system health
    console.log('2. Testing enhanced system health...');
    const health = await testSystemHealth();
    
    // Test enhanced LLM
    console.log('3. Testing enhanced LLM capabilities...');
    await testEnhancedLLM();
    
    // Test emergency fallback
    console.log('4. Testing emergency fallback...');
    await testEmergencyFallback();
    
    // Demonstrate LangChain
    console.log('5. Demonstrating LangChain integration...');
    await demonstrateLangChain();
    
    console.log('ðŸŽ‰ Enhanced LLM and LangChain Integration Complete!');
    console.log('===============================================');
    console.log('ðŸŒ Server: http://localhost:5000');
    console.log('ðŸ“Š Health: http://localhost:5000/health');
    console.log('ðŸ§  Enhanced LLM: http://localhost:5000/api/llm');
    console.log('ðŸš¨ Emergency: http://localhost:5000/api/emergency');
    console.log('ðŸ”— LangChain Test: http://localhost:5000/api/llm/test');
    console.log('ðŸ“š Knowledge Base: http://localhost:5000/api/llm/knowledge');
    console.log('');
    console.log('ðŸ§  ENHANCED FEATURES:');
    console.log('â€¢ LangChain integration with Python backend');
    console.log('â€¢ Knowledge base integration');
    console.log('â€¢ Fast response caching');
    console.log('â€¢ Emergency fallback system');
    console.log('â€¢ Math calculations');
    console.log('â€¢ Multiple fallback layers');
    console.log('â€¢ 100% guaranteed responses');
    console.log('â€¢ Ultra-fast response times');
    console.log('');
    console.log('ðŸš€ The VUAI Agent now has enhanced LLM capabilities with LangChain!');
};

// Handle process termination
process.on('SIGINT', () => {
    console.log('\nðŸ›‘ Enhanced LLM system shutdown');
    process.exit(0);
});

// Launch the enhanced system
launchEnhancedLLM().catch(console.error);
